# History Processor Examples

Comprehensive examples demonstrating Pydantic AI's conversation history management features. Learn how to build stateful AI agents that maintain context across multiple interactions.

## üìö Learning Path

Work through these examples in order to progressively understand history handling:

### 1. **Basic History Handling** (`1_basic_history_handling.py`)

**Concepts:** Message inspection, JSON serialization, object representation

- View conversation history in JSON format
- Access individual messages as objects
- Understand ModelMessage structure

```bash
uv run python 1_basic_history_handling.py
```

---

### 2. **Continuous History** (`2_continuous_history.py`)

**Concepts:** Multi-turn conversations, message_history parameter, context passing

- Build multi-turn conversations with agent
- Pass history to maintain context
- Inspect full conversation state
- Understand the difference between `new_messages()` and `all_messages()`

```bash
uv run python 2_continuous_history.py
```

---

### 3. **History Usage in Real Workflows** (`3_history_usage.py`)

**Concepts:** Conversation summarization, persistence, JSON serialization

- Create realistic multi-turn conversation flow
- Leverage history for conversation summarization
- Save conversation history to JSON
- Load persisted history for future use

```bash
uv run python 3_history_usage.py
```

---

### 4. **History Filtering** (`4_history_filtering.py`)

**Concepts:** History processors, filtering strategies, ModelRequest vs ModelResponse

- Filter history to only user messages (ModelRequest)
- Filter history to only model responses (ModelResponse)
- Understand constraints: history must end with ModelRequest
- Apply custom transformations to history

```bash
uv run python 4_history_filtering.py
```

---

### 5. **Context Window Management**

Split into two approaches:

#### 5a. **Fixed Message Limit** (`5a_history_length_fixed.py`)

**Concepts:** Message count limiting, simple truncation

- Keep only the last N messages
- Basic strategy for preventing history bloat
- Useful for simple use cases with small context windows

```bash
uv run python 5a_history_length_fixed.py
```

#### 5b. **Dynamic Token-Based Management** (`5b_history_length_dynamic.py`)

**Concepts:** Token estimation, RunContext dependency injection, stateful processing

- Estimate tokens consumed by messages
- Dynamic context guarding based on thresholds
- Advanced state management with dataclasses
- Production-ready patterns for token-aware history

```bash
uv run python 5b_history_length_dynamic.py
```

#### 5c. **History Trimming with Tool Calls** (`5c_history_with_tools.py`)

**Concepts:** Tool-call/response pair integrity, safe history slicing, agent tool integration

- Preserve tool-call and tool-response pairs during history trimming
- Understand why splitting tool pairs breaks agent execution
- Compare naive truncation vs. tool-aware truncation
- Practical patterns for agents that use tools

```bash
uv run python 5c_history_with_tools.py
```

---

### 6. **Persistent History with Database** (`6_persistent_history.py`)

**Concepts:** Database persistence, SQLite ORM, conversation archival

- Save conversation history to SQLite database
- Store metadata: prompts, responses, token usage, model information
- Retrieve and query historical conversations
- Build a conversation archive system
- Real-world patterns for durable storage

```bash
uv run python 6_persistent_history.py
```

---

## üîë Key Concepts

### Message Types

- **ModelRequest**: User/human messages sent to the agent
- **ModelResponse**: Responses generated by the AI model
- **ModelMessage**: Base type for any message in the conversation

### History Methods

- `result.new_messages()` - Only the messages from current inference
- `result.all_messages()` - Complete conversation history up to this point
- `result.all_messages_json()` - Serialized history as JSON bytes

### History Processors

Functions that transform history before sending to model:

```python
def my_processor(messages: list[ModelMessage]) -> list[ModelMessage]:
    """Transform history before agent processes it"""
    return [msg for msg in messages if isinstance(msg, ModelRequest)]

agent = Agent("openai:gpt-5.1", history_processors=[my_processor])
```

### Important Constraint ‚ö†Ô∏è

**History must always end with a ModelRequest (user message)**. If you filter to only ModelResponse messages, the agent cannot process the history.

---

## üöÄ Quick Start

1. **Setup environment:**

   ```bash
   cp .env.example .env
   # Edit .env and add your OPENAI_API_KEY
   ```

2. **Run examples in sequence:**

   ```bash
   uv run python 1_basic_history_handling.py
   uv run python 2_continuous_history.py
   uv run python 3_history_usage.py
   uv run python 4_history_filtering.py
   uv run python 5a_history_length_fixed.py
   uv run python 5b_history_length_dynamic.py
   uv run python 5c_history_with_tools.py
   uv run python 6_persistent_history.py
   ```

3. **Understand the progression:** Each example builds on concepts from previous ones.

---

## üí° Common Patterns

### Pattern 1: Stateful Conversation

```python
# First turn
result_1 = agent.run_sync("Your prompt here")

# Second turn with context
result_2 = agent.run_sync(
    "Follow-up question",
    message_history=result_1.new_messages()
)
```

### Pattern 2: History Filtering

```python
def user_message_filter(messages: list[ModelMessage]) -> list[ModelMessage]:
    return [msg for msg in messages if isinstance(msg, ModelRequest)]

agent = Agent("openai:gpt-5.1", history_processors=[user_message_filter])
```

### Pattern 3: Context Window Management

```python
def keep_last_messages(
    messages: list[ModelMessage],
    num_messages: int = 3
) -> list[ModelMessage]:
    return (
        messages[-num_messages:]
        if len(messages) > num_messages
        else messages
    )

agent = Agent("openai:gpt-5.1", history_processors=[keep_last_messages])
```

### Pattern 4: Database Persistence

```python
# Save conversation to database
record = prepare_data_for_db(prompt, result)
add_message_to_db(record)

# Retrieve conversation
conversation = get_conversation_by_id(record.id)
```

---

## üìù Dependencies

- `pydantic-ai` - Core framework
- `loguru` - Logging
- `python-dotenv` - Environment configuration
- `sqlalchemy` - Database ORM (for example 6)

See `pyproject.toml` for full dependencies.

---

## ‚ùì FAQ

**Q: Why split history messages with `new_messages()` instead of `all_messages()`?**
A: `new_messages()` is more efficient for stateless interactions. Use `all_messages()` when you need the complete conversation.

**Q: Can I use history without an API key?**
A: No, these examples require an OpenAI API key. Examples run against real models for demonstration purposes.

**Q: How do I handle very long conversations?**
A: Use strategy from example 5. Either:

- Keep only last N messages (5a)
- Estimate tokens and trim dynamically (5b)
- Summarize old messages using an LLM call

**Q: Can I save history to a database?**
A: Yes! See example 6 for a complete SQLite database persistence pattern with conversation archival and retrieval.

---

## üîß Environment Setup

Create `.env` file from `.env.example`:

```bash
OPENAI_API_KEY=sk-your-key-here
```

All examples load this automatically via `load_dotenv()`.

---

## üìñ Further Reading

- [Pydantic AI Documentation](https://ai.pydantic.dev/)
- [OpenAI API Docs](https://platform.openai.com/docs)
- [Context Window Limits by Model](https://platform.openai.com/docs/models)
